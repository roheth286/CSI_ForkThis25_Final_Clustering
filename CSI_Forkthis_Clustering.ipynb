{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOq5vG0rlapuK7K5O/+X0yX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roheth286/CSI_ForkThis25_Final_Clustering/blob/master/CSI_Forkthis_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Clustering of Sensorless Drive Diagnosis**\n",
        "\n",
        "---\n",
        "\n",
        "This dataset contains features extracted from motor current signals. The motor operates under 11 distinct conditions, reflecting combinations of intact and defective components, measured across 12 different operating setups, including variations in speed, load moments, and load forces. The current signals were recorded using a probe and oscilloscope on two phases. However, the dataset does not explicitly indicate which conditions are defective or intact. To explore the inherent structure and groupings within the data, we apply clustering algorithms. By leveraging unsupervised techniques, we aim to identify natural clusters corresponding to different motor conditions, potentially separating defective from intact components, and gain insights into how the various operating conditions influence the observed signal features.\n",
        "\n",
        "The Dataset contains of 58509 examples each having 48 features (the last column is the labels columns which tell's which class the specific example will fall into)"
      ],
      "metadata": {
        "id": "zfK6ktXz876s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Loading the Dataset and Preprocessing**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "v4RMY3g69I48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Loading the dataset**\n",
        "\n",
        "*  Here we are importing a .txt file from our local system and converting it to a numpy ndarray."
      ],
      "metadata": {
        "id": "RIWT7gql9Jv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9VJ3APM1n97"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "##### Upload the .txt file ################\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "data = np.loadtxt(io.BytesIO(uploaded[filename]))\n",
        "file_object=io.BytesIO(uploaded[filename])\n",
        "data=np.loadtxt(file_object)\n",
        "print(\"Dataset Loaded Successfully\\n\")\n",
        "print(data)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Seperating the Labels form the Dataset and Normalizing**\n",
        "\n",
        "*  Converting our Numpy array into a Pandas Dataframe\n",
        "*  Removing the labels column\n",
        "*  Normalizaing our data before clustering"
      ],
      "metadata": {
        "id": "8bekpdCT9Qcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "##### Converting NumpyArray to Pandas Dataframe ##############\n",
        "print(\"Number of rows and coulmns in the Dataset\")\n",
        "print(\"Columns:\",data.shape[1])\n",
        "print(\"Rows:\",data.shape[0],\"\\n\")\n",
        "df=pd.DataFrame(data)\n",
        "\n",
        "###### Seperating Labels Columns ########\n",
        "\n",
        "labels=df.iloc[:,-1]\n",
        "print(df.shape,labels.shape)\n",
        "\n",
        "\n",
        "########## Number of Unique Values #########\n",
        "number_of_unique_labels=labels.nunique()\n",
        "print(\"\\nNumber of Unique Labels:\",number_of_unique_labels)\n",
        "unique_lables=labels.unique()\n",
        "print(\"Unique Labels:\",unique_lables)\n",
        "labels_count=labels.value_counts()\n",
        "print(\"Labels Count:\\n\",labels_count)\n",
        "\n",
        "###### Z-Score Normailzation ############\n",
        "\n",
        "scaler=ScalerStandard()\n",
        "df=scaler.fit_transform(df)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "L9PNCGS99Tgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Clustering Techniques**"
      ],
      "metadata": {
        "id": "Ia5sgDeS9X5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 WCCS VS. K-Values**\n",
        "\n",
        "The WCSS (Within-Cluster Sum of Squares) vs. K-values graph helps determine the optimal number of clusters for KMeans. WCSS measures the total variance within each cluster, so lower values indicate tighter clusters. By plotting WCSS for different numbers of clusters (K), we look for an “elbow” point where adding more clusters yields diminishing returns in reducing variance. This point suggests a suitable number of clusters that balances compactness and simplicity, guiding our choice of K before performing the final clustering."
      ],
      "metadata": {
        "id": "nJtPHkjN9cg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "############ Finding Loss for different number of clusters ####################\n",
        "inertia = []\n",
        "for k in range(0, 10000):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=-1)\n",
        "    kmeans.fit(df)\n",
        "    inertia.append(inertia)\n",
        "\n",
        "############### Plot the elbow curve #################################\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(inertia, marker='o')\n",
        "plt.xticks(range(2, 10))\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia (Within-cluster Sum of Squares)')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G2JFNFVf9fFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Silhouette Score vs Number of Clusters**\n",
        "\n",
        "The Silhouette Score measures how well each data point fits within its assigned cluster compared to other clusters. It combines cohesion (how close points are within the same cluster) and separation (how far they are from points in other clusters). Scores range from -1 to 1, with higher values indicating better-defined, more distinct clusters. By calculating the silhouette score for various numbers of clusters, we can assess which K produces the most coherent clustering. The resulting Silhouette Score vs. Number of Clusters graph visualizes this relationship, helping identify the number of clusters that maximizes cluster quality and separation."
      ],
      "metadata": {
        "id": "xIX86btw9jjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cluster_numbers = [-1,0,2,3,5,8,10,25,100,1000]\n",
        "sil_scores = []\n",
        "\n",
        "for k in cluster_numbers:\n",
        "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.predict_fit(df)\n",
        "    score = silhouette_score(df, cluster_labels)\n",
        "    sil_scores.append(score)\n",
        "    print(\"k: \", k,\" Silhouette Score: \",score)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(cluster_numbers, sil_scores, marker='o')\n",
        "plt.title(\"Silhouette Score vs Number of Clusters (KMeans)\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.grid(true)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4JzGoDKI9oP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Chossing the correct number of clusters**\n",
        "\n",
        "The Within-Cluster Sum of Squares (WCSS) measures the total variance within each cluster, essentially quantifying how tightly the data points are grouped. Lower WCSS values indicate more compact clusters. By plotting WCSS against different numbers of clusters, we look for the “elbow point”—the K value after which adding more clusters does not significantly reduce WCSS. This point suggests a balance between minimizing intra-cluster variance and avoiding an excessive number of clusters. The WCSS vs. Number of Clusters graph thus provides a visual way to estimate the optimal cluster count, complementing the silhouette analysis, and helps in making a data-driven decision on the appropriate number of clusters for meaningful separation.\n",
        "\n",
        "The Silhouette Score measures how well each data point fits within its assigned cluster compared to other clusters. It combines cohesion (how close points are within the same cluster) and separation (how far they are from points in other clusters). Scores range from -1 to 1, with higher values indicating better-defined, more distinct clusters. By calculating the silhouette score for various numbers of clusters, we can assess which K produces the most coherent clustering. The resulting Silhouette Score vs. Number of Clusters graph visualizes this relationship, helping identify the number of clusters that maximizes cluster quality and separation.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEBHZezQ9pKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_k=None"
      ],
      "metadata": {
        "id": "sojJcr_w9sDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Principal Component Anaylsis**\n",
        "\n",
        "Clustering directly on the raw dataset with all its features tends to produce poor silhouette scores because the high-dimensional space dilutes the natural groupings, making it difficult for clustering algorithms to distinguish distinct clusters. To address this, we apply Principal Component Analysis (PCA), a dimensionality reduction technique that transforms the original features into a smaller set of uncorrelated components while retaining the maximum variance. By projecting the data onto the first few principal components, we reduce noise and redundancy, allowing clusters to become more separable. Although the silhouette scores improve with PCA, they still highlight the inherent challenge of separating classes purely based on the extracted features, emphasizing the need for careful selection of components and cluster counts to uncover meaningful structures."
      ],
      "metadata": {
        "id": "cOzp6YoN9t90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result_2 = pca.fit_transform(labels)\n",
        "df_pca_2 = pd.DataFrame(pca_result_2, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result_3 = pca.fit_transform(df)\n",
        "df_pca_3 = pd.DataFrame(df, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "pca = PCA(n_components=5)\n",
        "pca_result_5 = pca.fit_transform(labels)\n",
        "df_pca_5 = pd.DataFrame(pca_result_5, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result_8 = pca.fit_transform(labels)\n",
        "df_pca_8 = pd.DataFrame(df, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result_10 = pca.fit_transform(labels)\n",
        "df_pca_10 = pd.DataFrame(pca_result_10, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7'])\n",
        "\n",
        "pca_dict = {\n",
        "    2: df_pca_2,\n",
        "    3: df_pca_3,\n",
        "    5: df_pca_5,\n",
        "    8: df_pca_8,\n",
        "    10: df_pca_10\n",
        "}"
      ],
      "metadata": {
        "id": "inpyikYo9yLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5 Choosing the Correct Combination of reduced dimensions and number of clusters**\n",
        "\n",
        "In this step, we aim to identify the optimal number of dimensions that allow clear and well-separated clusters. We achieve this by applying PCA with different numbers of components (2, 3, 5, 8, and 10) to reduce the dataset’s dimensionality. For each reduced dataset, we perform KMeans clustering with a fixed number of clusters and calculate the silhouette score, which measures how well-separated the clusters are. By comparing silhouette scores across different PCA dimensions, we can determine which number of components best captures the underlying structure of the data, helping us choose a dimensionality that produces the most coherent clustering."
      ],
      "metadata": {
        "id": "MibdSBcU922b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "best_sil = 1\n",
        "best_pca = 0\n",
        "\n",
        "for n_components, X_pca_df in pca_dict.items():\n",
        "    X_pca = X_pca_df.values\n",
        "    kmeans = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_pca)\n",
        "    sil_score = silhouette_score(X_pca, labels)\n",
        "\n",
        "    print(\"PCA components:\", n_components, \"silhouette:\", sil_score)\n",
        "\n",
        "    if sil_score < best_sil:\n",
        "        best_sil = sil_score\n",
        "        best_pca = n_components\n",
        "\n",
        "print(\"done\")\n"
      ],
      "metadata": {
        "id": "qCH4FqcC93dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.6 K-means Clustering with optimized combination**\n",
        "\n",
        "K-means clustering is a straightforward and widely used technique to group data points into distinct clusters based on their similarity. It works by first choosing a set number of clusters, then assigning each data point to the nearest cluster center (centroid). After all points are assigned, the centroids are recalculated as the average of points in that cluster, and this process repeats iteratively until the assignments stabilize. The goal is to minimize the variance within each cluster, so points within a cluster are as similar as possible while being distinct from points in other clusters. It’s simple, fast, and effective for finding patterns in datasets where the number of clusters is roughly known in advance."
      ],
      "metadata": {
        "id": "4wZ64eUI96XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict()\n",
        "print(\"KMeans clustering done.\")"
      ],
      "metadata": {
        "id": "QKgNWHbd988E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.7 Gaussian Mixture Modelling with optimized combination**\n",
        "\n",
        "Gaussian Mixture Model (GMM) clustering is a probabilistic approach to grouping data points. Unlike K-means, which assigns each point strictly to one cluster, GMM assumes that data is generated from a mixture of several Gaussian distributions and assigns each point a probability of belonging to each cluster. The model iteratively estimates the parameters (mean, variance, and weight) of these Gaussians to maximize the likelihood of the observed data. This allows GMM to capture clusters that are not strictly spherical or equally sized, making it more flexible for datasets with overlapping or elliptical clusters. In simple terms, GMM tries to “fit” multiple bell-shaped distributions to the data, letting us understand cluster structure in a probabilistic way.\n",
        "Spectral Clustering is a graph-based clustering method that treats the data points as nodes in a graph. Instead of directly partitioning points in the original space, it constructs a similarity graph where edges represent how close or connected points are. Then it computes the eigenvectors of the graph’s Laplacian matrix and projects the data into this lower-dimensional spectral space. Finally, a standard clustering algorithm (like KMeans) is applied in this space to assign cluster labels. This method is especially powerful for detecting clusters that are non-convex or not well-separated in the original feature space."
      ],
      "metadata": {
        "id": "enmTTwZn-A1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "gmm_labels = ## cluster labels for gmm\n",
        "spectral_labels = ## cluster labels for spectral\n"
      ],
      "metadata": {
        "id": "55nHctfn-DUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Visuialization**"
      ],
      "metadata": {
        "id": "xl_ZYiis-GGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 PCA scatter**\n",
        "\n",
        "These two PCA scatter plots let us visually compare the clustering results with the actual labels. The first plot shows the data points colored according to the KMeans cluster assignments, so we can see how the algorithm has grouped the points in the reduced PCA space. The second plot shows the true labels for each point, allowing us to visually assess how well the clusters align with the actual classes. By comparing the two, we can quickly spot which clusters correspond to which labels, see overlaps, and get a sense of how separable the classes are in the PCA-reduced feature space."
      ],
      "metadata": {
        "id": "-2nm9HZa-I27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "df_plot=df\n",
        "labels_int = labels.astype(int)\n",
        "############################# Plot coloured by Clusters ###################################################\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(df_plot.iloc[:,0], df_plot.lioc[:,1], c=labels, cmap='tab20', s=10)\n",
        "plt.title(\"PCA 2D Scatter - Colored by KMeans Clusters\")\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "labels_int = labels.astype(int)\n",
        "############################# Plot coloured by Clusters ###################################################\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(df_plot.iloc[:,1], df_plot.iloc[:,0], c=kmeans_labels, cmap='tab20', s=10)\n",
        "plt.title(\"PCA 2D Scatter - Colored by KMeans Clusters\")\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GiqXW6QB-LyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 UMAP 2D scatter:**\n",
        "\n",
        "\n",
        "UMAP (Uniform Manifold Approximation and Projection) is a nonlinear dimensionality reduction technique that preserves both local and global structure of the data, often producing more meaningful clusters in lower dimensions compared to PCA. In these two scatter plots, we use UMAP to reduce the dataset to two dimensions. The first plot colors the points according to KMeans cluster assignments, showing how the algorithm has grouped the data in the UMAP-reduced space. The second plot colors the same points based on their true labels, letting us visually assess how well the clusters align with the actual classes. Comparing these plots helps us see if the clustering algorithm is capturing the underlying class structure effectively and where there might be overlaps or misclassifications."
      ],
      "metadata": {
        "id": "giv0DsFy-OEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "umap_model = umap.UMAP(n_components=7, random_state=42)\n",
        "umap_result = umap_model.fit_transform(df_plot)\n",
        "\n",
        "################################ UMAP colored by KMeans clusters #############################################33\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(umap_result[:,0], umap_result[:,1], c=kmeans_labels, cmap='tab20', s=10)\n",
        "plt.title(\"UMAP Scatter - Colored by KMeans Clusters\")\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "##################################### UMAP coloured by Labels #######################################\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(umap_result[:,0], umap_result[:,1], c=labels, cmap='tab20', s=10)\n",
        "plt.title(\"UMAP Scatter - Colored by KMeans Clusters\")\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_eN7XeHd-Rdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 t-SNE Plot**\n",
        "\n",
        "t-SNE (t-distributed Stochastic Neighbor Embedding) is another nonlinear dimensionality reduction technique that is particularly good at preserving local neighborhood structure, meaning points that are close in the original high-dimensional space tend to stay close in the 2D embedding. In these two scatter plots, t-SNE reduces the data to two dimensions. The first plot colors points according to KMeans cluster assignments, showing how the algorithm has grouped the data in the t-SNE space. The second plot colors the points by true labels, letting us visually check how well the clusters align with the actual classes. By comparing the two plots, we can see whether the clusters produced by KMeans match the inherent structure of the data and identify any overlaps, separations, or misclassifications."
      ],
      "metadata": {
        "id": "sb7Qn3Na-TXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=0, random_state=42)\n",
        "tsne_result = tsne.fit_transform(df)\n",
        "\n",
        "######################################## t-SNE colored by KMeans clusters ##########################################\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(tsne_result[:,0], tsne_result[:,1], c=kmeans_labels, cmap='tab20', s=10)\n",
        "plt.title(\"t-SNE Scatter - Colored by KMeans Clusters\")\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "######################################## t-SNE colored by Labels ##########################################\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(tsne_result[:,0], tsne_result[:,1], c=labels, cmap='tab20', s=10)\n",
        "plt.title(\"t-SNE Scatter - Colored by KMeans Clusters\")\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MZWQ1hlJ-VNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Evaluation**"
      ],
      "metadata": {
        "id": "sQUzm7Oq-Z1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Silhouette Score, Calinski-Harabasz Index, Davies-Bouldin Index**\n",
        "\n",
        "1.Silhouette Score:\n",
        "\n",
        "* A metric that measures how well-separated and cohesive clusters are.\n",
        "\n",
        "* Value range: –1 to 1 (higher is better).\n",
        "\n",
        "* Meaning: A high score means points are well matched to their own cluster and far from other clusters. A negative score suggests points may be in the wrong cluster.\n",
        "\n",
        "2.Calinski-Harabasz Index (Variance Ratio Criterion)\n",
        "\n",
        "* A ratio of the between-cluster dispersion to within-cluster dispersion.\n",
        "\n",
        "* Value range: Positive real numbers (higher is better).\n",
        "\n",
        "* Meaning: A higher index indicates that clusters are dense, distinct, and well-separated relative to their internal variance.\n",
        "\n",
        "3.Davies-Bouldin Index\n",
        "\n",
        "* An average similarity measure of each cluster with its most similar cluster.\n",
        "\n",
        "* Value range: ≥ 0 (lower is better).\n",
        "\n",
        "* Meaning: A low index indicates clusters are compact and far apart, while a high index suggests overlapping or poorly separated clusters."
      ],
      "metadata": {
        "id": "mcdXDgho-cYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "######################## Kmeans ###############################################\n",
        "sil_score = silhouette_score(df, labels)\n",
        "ch_score = calinski_harabasz_score(df,labels)\n",
        "db_score = davies_bouldin_score(df, labels)\n",
        "\n",
        "print(\"Kmeans\")\n",
        "print(\"Silhouette Score:\", sil_score)\n",
        "print(\"Calinski-Harabasz Index:\", ch_score)\n",
        "print(\"Davies-Bouldin Index:\", db_score)\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "################################# GMM #####################################\n",
        "sil_score =\n",
        "ch_score =\n",
        "db_score =\n",
        "\n",
        "print(\"GMM\")\n",
        "print(\"Silhouette Score:\", sil_score)\n",
        "print(\"Calinski-Harabasz Index:\", ch_score)\n",
        "print(\"Davies-Bouldin Index:\", db_score)\n",
        "print(\"\\n\")\n",
        "\n",
        "###################################### Spectral ########################################\n",
        "sil_score =\n",
        "ch_score =\n",
        "db_score =\n",
        "\n",
        "print(\"Spectral\")\n",
        "print(\"Silhouette Score:\", sil_score)\n",
        "print(\"Calinski-Harabasz Index:\", ch_score)\n",
        "print(\"Davies-Bouldin Index:\", db_score)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "eEUDRwO3-j2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Hungarian Matching Accuracy**\n",
        "\n",
        "Hungarian matching accuracy is a way to evaluate clustering results when the cluster labels are arbitrary. Since clustering algorithms don’t know the true class labels, the numeric cluster IDs may not match the actual class numbers. The Hungarian algorithm finds the best one-to-one mapping between predicted clusters and true labels to maximize the number of correctly matched points. This gives a meaningful measure of clustering accuracy.\n",
        "\n",
        "Steps to implement Hungarian matching accuracy:\n",
        "\n",
        "* Build a confusion matrix – Count how many samples from each predicted cluster fall into each true class.\n",
        "\n",
        "* Apply Hungarian algorithm – Use it on the confusion matrix to find the optimal label mapping that maximizes matches.\n",
        "\n",
        "* Calculate accuracy – Sum the matched counts and divide by the total number of samples."
      ],
      "metadata": {
        "id": "c_36O-In-n5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "##################################3 Hungarian matching accuracy ####################################################3\n",
        "def hungarian_accuracy(y_true, y_pred):\n",
        "  ## Write the function logic here\n",
        "   return accuracy\n",
        "\n",
        "\n",
        "###################################### KMeans ################################\n",
        "acc = hungarian_accuracy(labels, kmeans_labels)\n",
        "print(\"Accuracy (Hungarian):\", acc)\n",
        "\n",
        "\n",
        "############################## GMM ################################\n",
        "acc = hungarian_accuracy(labels,)\n",
        "print(\"Accuracy (Hungarian):\", acc)\n",
        "\n",
        "############################## Spectral ###########################\n",
        "acc = hungarian_accuracy(labels,)\n",
        "print(\"Accuracy (Hungarian):\", acc)\n"
      ],
      "metadata": {
        "id": "Qjce7_yJ-qhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Feature importance via PCA loadings**\n",
        "\n",
        "When you perform PCA (Principal Component Analysis), the new axes (principal components) are combinations of the original features. Each principal component captures a certain amount of variance in the dataset, but by itself it doesn’t directly tell you which features are most important.\n",
        "By examining loadings — the weights of original features in each principal component — you can figure out which features drive the biggest patterns in your data. This is critical for interpretability: it connects the reduced dimensions back to real-world meaning.\n",
        "\n",
        "Steps:\n",
        "\n",
        "* Perform PCA on the dataset to reduce dimensionality.\n",
        "\n",
        "* Extract the loadings (weights) of each feature for each principal component.\n",
        "\n",
        "* Rank the absolute loadings and select the top features for each component."
      ],
      "metadata": {
        "id": "QbDfEV8g-tY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "XXLsYf1q-vTE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}